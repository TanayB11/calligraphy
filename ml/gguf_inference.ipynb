{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/mistral_textfx/mistral_similes-Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.65 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4765.49 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '16', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': '.'}\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path='models/mistral_textfx/mistral_similes-Q5_K_S.gguf',\n",
    "    chat_format=\"mistral-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9243.18 ms\n",
      "llama_print_timings:      sample time =       1.92 ms /    19 runs   (    0.10 ms per token,  9921.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2065.09 ms /    19 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =    2098.31 ms /    20 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-2a924034-bc09-49f6-82c3-ba4394b70678',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709580675,\n",
       " 'model': 'models/mistral_textfx/mistral_similes-Q5_K_S.gguf',\n",
       " 'choices': [{'text': ' is the gentle blanket that wraps around my mind, calming its restless thoughts.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 18, 'total_tokens': 28}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_completion('Create a simile for this concept: Sleep', max_tokens=512, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanay/Documents/Programming/calligraphy/ml/gguf_inference.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanay/Documents/Programming/calligraphy/ml/gguf_inference.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mcreate_chat_completion([\u001b[39m'\u001b[39;49m\u001b[39mGive me a recipe for pizza\u001b[39;49m\u001b[39m'\u001b[39;49m], max_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/llama_cpp/llama.py:1638\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m \n\u001b[1;32m   1604\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[39m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m handler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_handler \u001b[39mor\u001b[39;00m llama_chat_format\u001b[39m.\u001b[39mget_chat_completion_handler(\n\u001b[1;32m   1636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_format\n\u001b[1;32m   1637\u001b[0m )\n\u001b[0;32m-> 1638\u001b[0m \u001b[39mreturn\u001b[39;00m handler(\n\u001b[1;32m   1639\u001b[0m     llama\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1640\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m   1641\u001b[0m     functions\u001b[39m=\u001b[39;49mfunctions,\n\u001b[1;32m   1642\u001b[0m     function_call\u001b[39m=\u001b[39;49mfunction_call,\n\u001b[1;32m   1643\u001b[0m     tools\u001b[39m=\u001b[39;49mtools,\n\u001b[1;32m   1644\u001b[0m     tool_choice\u001b[39m=\u001b[39;49mtool_choice,\n\u001b[1;32m   1645\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1646\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1647\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1648\u001b[0m     min_p\u001b[39m=\u001b[39;49mmin_p,\n\u001b[1;32m   1649\u001b[0m     typical_p\u001b[39m=\u001b[39;49mtypical_p,\n\u001b[1;32m   1650\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1651\u001b[0m     stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1652\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1653\u001b[0m     response_format\u001b[39m=\u001b[39;49mresponse_format,\n\u001b[1;32m   1654\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1655\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1656\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1657\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1658\u001b[0m     tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1659\u001b[0m     mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1660\u001b[0m     mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1661\u001b[0m     mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1662\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1663\u001b[0m     logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1664\u001b[0m     grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1665\u001b[0m     logit_bias\u001b[39m=\u001b[39;49mlogit_bias,\n\u001b[1;32m   1666\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/llama_cpp/llama_chat_format.py:330\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchat_completion_handler\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m    296\u001b[0m     llama: llama\u001b[39m.\u001b[39mLlama,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m     Iterator[llama_types\u001b[39m.\u001b[39mCreateChatCompletionStreamResponse],\n\u001b[1;32m    329\u001b[0m ]:\n\u001b[0;32m--> 330\u001b[0m     result \u001b[39m=\u001b[39m chat_formatter(\n\u001b[1;32m    331\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m    332\u001b[0m         functions\u001b[39m=\u001b[39;49mfunctions,\n\u001b[1;32m    333\u001b[0m         function_call\u001b[39m=\u001b[39;49mfunction_call,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     prompt \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mprompt\n\u001b[1;32m    336\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mstop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/llama_cpp/llama_chat_format.py:928\u001b[0m, in \u001b[0;36mformat_mistral_instruct\u001b[0;34m(messages, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m prompt \u001b[39m=\u001b[39m bos\n\u001b[1;32m    926\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m messages:\n\u001b[1;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m--> 928\u001b[0m         message[\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m         \u001b[39mand\u001b[39;00m message[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    930\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(message[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mstr\u001b[39m)\n\u001b[1;32m    931\u001b[0m     ):\n\u001b[1;32m    932\u001b[0m         prompt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[INST] \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m message[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    934\u001b[0m         message[\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    935\u001b[0m         \u001b[39mand\u001b[39;00m message[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(message[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mstr\u001b[39m)\n\u001b[1;32m    937\u001b[0m     ):\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "llm.create_completion('Give me a recipe for pizza', max_tokens=512, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
