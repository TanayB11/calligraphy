{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanay/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference w/ LoRA on Mistral7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, LoftQConfig, get_peft_model, PeftModel\n",
    "from datasets import Dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.45s/it]"
     ]
    }
   ],
   "source": [
    "# can change the quantization type using\n",
    "# https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanay/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanay/Documents/Programming/calligraphy/ml/textfx/mistral_textfx_inference.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanay/Documents/Programming/calligraphy/ml/textfx/mistral_textfx_inference.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load existing adapter\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanay/Documents/Programming/calligraphy/ml/textfx/mistral_textfx_inference.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39;49mfrom_pretrained(base_model, \u001b[39m'\u001b[39;49m\u001b[39mmistral_similes/textfx_adapter\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/peft_model.py:353\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    352\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39;49mtask_type](model, config, adapter_name)\n\u001b[1;32m    354\u001b[0m model\u001b[39m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39mis_trainable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/peft_model.py:1051\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, peft_config, adapter_name)\n\u001b[1;32m   1052\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/peft_model.py:127\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_peft_config \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39mpeft_type]\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(model, {adapter_name: peft_config}, adapter_name)\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mis_gradient_checkpointing\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/model.py:109\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, config, adapter_name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, config, adapter_name)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:148\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\u001b[39m.\u001b[39mupdate(peft_config)\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter \u001b[39m=\u001b[39m adapter_name\n\u001b[0;32m--> 148\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minject_adapter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, adapter_name)\n\u001b[1;32m    150\u001b[0m \u001b[39m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpeft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:303\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    301\u001b[0m     is_target_modules_in_base_model \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     parent, target, target_name \u001b[39m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001b[39m=\u001b[39;49mkey)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m    306\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    307\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget modules \u001b[39m\u001b[39m{\u001b[39;00mpeft_config\u001b[39m.\u001b[39mtarget_modules\u001b[39m}\u001b[39;00m\u001b[39m not found in the base model. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease check the target modules and try again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/model.py:176\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    167\u001b[0m     target\u001b[39m.\u001b[39mupdate_layer(\n\u001b[1;32m    168\u001b[0m         adapter_name,\n\u001b[1;32m    169\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         lora_config\u001b[39m.\u001b[39muse_rslora,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     new_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_module(lora_config, adapter_name, target, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m adapter_name \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter:\n\u001b[1;32m    178\u001b[0m         \u001b[39m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    179\u001b[0m         new_module\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/model.py:251\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m new_module \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m dispatcher \u001b[39min\u001b[39;00m dispatchers:\n\u001b[0;32m--> 251\u001b[0m     new_module \u001b[39m=\u001b[39m dispatcher(target, adapter_name, lora_config\u001b[39m=\u001b[39;49mlora_config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m new_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# first match wins\u001b[39;00m\n\u001b[1;32m    253\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/layer.py:708\u001b[0m, in \u001b[0;36mdispatch_default\u001b[0;34m(target, adapter_name, lora_config, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mfan_in_fan_out\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lora_config\u001b[39m.\u001b[39mfan_in_fan_out \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(lora_config\u001b[39m.\u001b[39mloftq_config)\n\u001b[0;32m--> 708\u001b[0m     new_module \u001b[39m=\u001b[39m Linear(target, adapter_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    709\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(target_base_layer, Conv1D):\n\u001b[1;32m    710\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mfan_in_fan_out\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/layer.py:211\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, base_layer, adapter_name, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, init_lora_weights, use_rslora, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfan_in_fan_out \u001b[39m=\u001b[39m fan_in_fan_out\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_adapter \u001b[39m=\u001b[39m adapter_name\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora)\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_target_conv_1d_layer \u001b[39m=\u001b[39m is_target_conv_1d_layer\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/layer.py:98\u001b[0m, in \u001b[0;36mLoraLayer.update_layer\u001b[0;34m(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling[adapter_name] \u001b[39m=\u001b[39m lora_alpha \u001b[39m/\u001b[39m r\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m init_lora_weights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mloftq\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloftq_init(adapter_name)\n\u001b[1;32m     99\u001b[0m \u001b[39melif\u001b[39;00m init_lora_weights:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_lora_parameters(adapter_name, init_lora_weights)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/tuners/lora/layer.py:143\u001b[0m, in \u001b[0;36mLoraLayer.loftq_init\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_base_layer()\u001b[39m.\u001b[39mweight\n\u001b[1;32m    137\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    138\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnum_bits\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mloftq_bits\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m),\n\u001b[1;32m    139\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreduced_rank\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr[adapter_name],\n\u001b[1;32m    140\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnum_iter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mloftq_iter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m    141\u001b[0m }\n\u001b[0;32m--> 143\u001b[0m qweight, lora_A, lora_B \u001b[39m=\u001b[39m loftq_init(weight, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m adapter_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_A\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    145\u001b[0m     \u001b[39m# initialize A the same way as the default for nn.Linear and B to zero\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_A[adapter_name]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m lora_A\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/peft/utils/loftq_utils.py:207\u001b[0m, in \u001b[0;36mloftq_init\u001b[0;34m(weight, num_bits, reduced_rank, num_iter)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     compute_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 207\u001b[0m weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mcompute_device, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m    208\u001b[0m res \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iter):\n",
      "File \u001b[0;32m~/Documents/Programming/calligraphy/env/lib/python3.8/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# load existing adapter\n",
    "model = PeftModel.from_pretrained(base_model, '../models/mistral_similes/textfx_adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer_function = lambda x : tokenizer(x['text'], truncation=True, padding='max_length', max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(prompt):\n",
    "     inputs = tokenizer(\n",
    "          prompt,\n",
    "          add_special_tokens=False,\n",
    "          return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "     outputs = model.generate(\n",
    "          inputs,\n",
    "          max_length=100,\n",
    "          do_sample=True,\n",
    "          top_k=5,\n",
    "          top_p=0.95,\n",
    "          temperature=0.7,\n",
    "          num_return_sequences=3)\n",
    "\n",
    "     generated = tokenizer.decode(outputs[0])\n",
    "     return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A good simile contains a concrete image that illustrates the concept we want to convey without being too obvious. Good similes are unexpected and evocative. Create a simile that illustrates this concept: hunger.\\n\\nA: The hunger gnaws at my insides like a wild beast.\\nB: My stomach growls like a lion, reminding me of its empty lair.\\nC: My hunger is a ravenous beast, devouring my thoughts and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similes_test = 'A good simile contains a concrete image that illustrates the concept we want to convey without being too obvious. Good similes are unexpected and evocative. Create a simile that illustrates this concept: hunger.'\n",
    "run_inference(similes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
