\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Calligraphy: Local LLMs as Writing Assistants}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Tanay Biradar\\
  UC Santa Barbara\\
  \texttt{tbiradar@ucsb.edu} \\
  \And
  Mateo Wang\\
  UC Santa Barbara\\
  \texttt{mathewwang@ucsb.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  In this work, we present "Calligraphy," a system designed to augment the
  capabilities of local Large Language Models (LLMs) as writing assistants.
  Our approach focuses on two main tasks: creative rewording and writing
  critiques. We leverage techniques such as fine-tuning on domain-specific
  data, employing Low-Rank Adapters (LoRA), and post-training quantization to
  enhance the performance of LLMs while maintaining efficiency. Our
  evaluation demonstrates that our system outperforms baseline models in
  terms of perplexity, a measure of language model fluency, and provides
  qualitative improvements in creative writing tasks.
\end{abstract}


\section{Introduction}


The advent of LLMs has revolutionized the field of natural language processing,
providing tools capable of conversational interaction, customization, and
multi-task learning. However, these models are not without their flaws, such as a
propensity for hallucinations, the need for extensive prompt engineering, and
occasional flawed reasoning. Our project, Calligraphy, aims to address these issues
by focuing on enhancing user writing, instead of generating writing for them,
playing up to LLMs' strengths of being able to creatively reword and critique
existing writing, delegating the task of generating substantive, factual content to
the user.


\section{Related works}
\label{rel_works}

The development of our project, Calligraphy, is informed by a rich landscape of
prior work that explores the use of Large Language Models (LLMs) as writing
assistants and creative tools. In this section, we review several key contributions that have influenced our approach.

\subsection{Maggie Appleton's Language Model Sketchbook}

Maggie Appleton's Language Model Sketchbook explores non-chatbot interfaces for
language models, proposing alternative interaction paradigms such as "Daemons" and
"Branches" to assist in writing and reasoning tasks. These concepts emphasize the
role of language models as "epistemic rubber ducks," serving as reflective thinking
partners rather than mere conversational agents. However, this is just a design of
an app and not an implementation. Our project, Calligraphy, actually implements
the idea of leveraging LLMs as reflective writing assistants, focusing on enhancing
writing critiques and creative rewording tasks.

\subsection{DeepL Write}

DeepL Write represents a practical implementation of LLMs in writing assistance,
focusing on aiding users in refining their writing across various languages. It
offers suggestions for grammar and punctuation corrections, tone adjustments, and
creative rephrasing, aiming to enhance clarity, precision, and expressiveness in
written communication. However, its scope is quite limited. Our project,
Calligraphy, aims to build upon this idea by automatically generating writing
critiques as one writes and allowing users to manually pick which word or phrase
they'd like to reword.

\subsection{TextFX}

TextFX is an AI experiment that utilizes Google's PaLM 2 LLM to provide a suite of tools aimed at improving and diversifying writing. It can generate literary devices such as synonyms, similes, metaphors, and alliterations, but is limited to short phrases and sentences. Our tool builds upon this to handle longer and more varied forms of writing.

\subsection{NotebookLM}

NotebookLM is a project that aims to integrate language models into research and non-fiction writing processes[3]. It offers functionalities such as rephrasing sentences, offering critiques, finding evidence for claims, and generating research questions, aligning with our project's goal of enhancing writing critiques.

\subsection{The Inquisitive Code Editor}

The Inquisitive Code Editor is a concept that leverages language models to assist in the coding process by suggesting revisions, fetching evidence, and elaborating on points[3]. Although it is more focused on coding, the underlying idea of using language models as assistants in creative tasks is shared with our project.

\subsection{LangChain}

LangChain is a framework that enables the integration of language models with search APIs to fetch relevant sources and support information retrieval[1]. Our project utilizes LangChain to implement the task of fetching sources in agreement or conflict, which is crucial for our writing critique component.

\subsection{GPT-3.5}

GPT-3.5, developed by OpenAI, is a state-of-the-art language model known for its conversational abilities, natural language understanding, and generation, as well as reasoning capabilities[1]. Our project uses GPT-3.5 to generate data for fine-tuning and as a benchmark for evaluating the performance of our enhanced models.

These related works provide a foundation and context for our project, Calligraphy, which aims to augment local LLMs as writing assistants by focusing on creative rewording and writing critiques. Our approach builds upon and extends the capabilities presented in these projects, contributing to the evolving field of language model applications.

\section{Method}
\label{method}




\section{Evaluation}
\label{eval}


\section{Preparing PDF files}


\section{Supplementary Material}

Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.


\section*{References}


References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


  [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
    Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}