\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Calligraphy: Local LLMs as Writing Assistants}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Tanay Biradar\\
  UC Santa Barbara\\
  \texttt{tbiradar@ucsb.edu} \\
  \And
  Mateo Wang\\
  UC Santa Barbara\\
  \texttt{mathewwang@ucsb.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  In this work, we present "Calligraphy," a system designed to augment the
  capabilities of local Large Language Models (LLMs) as writing assistants.
  Our approach focuses on two main tasks: creative rewording and writing
  critiques. We leverage techniques such as fine-tuning on domain-specific
  data, employing Low-Rank Adapters (LoRA), and post-training quantization to
  enhance the performance of LLMs while maintaining efficiency. Our
  evaluation demonstrates that our system outperforms baseline models in
  terms of perplexity, a measure of language model fluency, and provides
  qualitative improvements in creative writing tasks.
\end{abstract}


\section{Introduction}


The advent of LLMs has revolutionized the field of natural language processing,
providing tools capable of conversational interaction, customization, and
multi-task learning. However, these models are not without their flaws, such as a
propensity for hallucinations, the need for extensive prompt engineering, and
occasional flawed reasoning. Our project, Calligraphy, aims to address these issues
by focuing on enhancing user writing, instead of generating writing for them,
playing up to LLMs' strengths of being able to creatively reword and critique
existing writing, delegating the task of generating substantive, factual content to
the user. We discuss the development process, including the implementation of a data
pipeline, fine-tuning techniques, and the integration of the system with a user
interface. Our evaluation shows that the system improves the quality of creative
writing, as evidenced by lower perplexity scores and positive user feedback.


\section{Related works}
\label{rel_works}

The development of our project, Calligraphy, is informed by a rich landscape of
prior work that explores the use of Large Language Models (LLMs) as writing
assistants and creative tools. In this section, we review several key
contributions that have influenced our approach.

\subsection{Maggie Appleton's Language Model Sketchbook}

Maggie Appleton's Language Model Sketchbook explores non-chatbot interfaces for
language models, proposing alternative interaction paradigms such as "Daemons" and
"Branches" to assist in writing and reasoning tasks. These concepts emphasize the
role of language models as "epistemic rubber ducks," serving as reflective thinking
partners rather than mere conversational agents. However, this is just a design of
an app and not an implementation. Our project, Calligraphy, actually implements
the idea of leveraging LLMs as reflective writing assistants, focusing on enhancing
writing critiques and creative rewording tasks.

\subsection{DeepL Write}

DeepL Write represents a practical implementation of LLMs in writing assistance,
focusing on aiding users in refining their writing across various languages. It
offers suggestions for grammar and punctuation corrections, tone adjustments, and
creative rephrasing, aiming to enhance clarity, precision, and expressiveness in
written communication. However, its scope is quite limited. Our project,
Calligraphy, aims to build upon this idea by automatically generating writing
critiques as one writes and allowing users to manually pick which word or phrase
they'd like to reword.

\subsection{TextFX}

TextFX is an AI experiment that utilizes Google's PaLM 2 LLM to provide a suite of
tools aimed at improving and diversifying writing. It can generate literary
devices such as synonyms, similes, metaphors, and alliterations, but is limited to
short phrases and sentences. Our tool builds upon this to handle longer and more
varied forms of writing.

\subsection{NotebookLM}

NotebookLM is a project uses language models to aid with research and non-fiction
writing. It offers functionalities such as rephrasing sentences, offering critiques,
finding evidence for claims, and generating research questions, aligning with our
project's goal of enhancing writing critiques. Our project builds on NotebookLM;
NotebookLM takes an innovative approach but requires uploading personal documents,
raising privacy concerns, which our project addresses by running locally.

\subsection{The Inquisitive Code Editor}

The Inquisitive Code Editor is a concept that leverages language models to assist in
the coding process by suggesting revisions, fetching evidence, and elaborating on
points. Although it is more focused on coding, it shares the underlying idea of using
language models as assistants in creative tasks.

\section{Method}
\label{method}

\subsection{Data Collection and Preprocessing}

We began by collecting a dataset of creative rewording examples using the TextFX tool and scraping data via Selenium. The dataset was preprocessed to fit an instruction template suitable for fine-tuning our models.

\subsection{Data Collection and Preprocessing}

We generated a dataset using GPT-3.5 and TextFX, focusing on creative rewording tasks. The data was preprocessed and structured into a standardized format suitable for fine-tuning.

\subsection{Model Fine-Tuning}

We fine-tuned general-purpose LLMs such as Mistral and Phi-2 using LoRA, a method that introduces low-rank matrices to adapt large pre-trained models to specific tasks without extensive retraining. We experimented with various hyperparameters to optimize the training process.

\subsection{Post-Training Quantization}

To address the issue of model size and enable local deployment, we applied post-training quantization techniques, reducing the model size from 25GB to 5GB without significant loss in performance.


\section{Evaluation}
\label{eval}

\subsection{Perplexity}

We evaluated our fine-tuned models using perplexity as a metric, which measures how well a probability model predicts a sample. Our LoRA-enhanced model achieved a perplexity of 6.9580, outperforming the base model's 7.3494, with a statistically significant p-value of 2.19e-5.

\subsection{Qualitative Analysis}

We conducted a qualitative analysis of the creative rewording and writing critiques generated by our system. The results showed that our system could produce more diverse and imaginative text compared to the base models. We also found that the writing critiques were more insightful and constructive, providing valuable feedback to the user.


\section{Conclusion}
\label{conclusion}


Calligraphy represents a significant step forward in the application of LLMs as writing assistants. The system not only aids in creative rewording but also provides valuable critiques that can improve the overall quality of writing. Our findings suggest that with further development, tools like Calligraphy could become indispensable for writers seeking to enhance their craft. This outline provides a structured approach to reporting the project in a concise manner, adhering to the NeurIPS 2023 style and page limits. The actual report would include more detailed explanations, figures, and tables to illustrate the methodology and results, as well as references to relevant literature.


\section*{References}


References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


  [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
    Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}